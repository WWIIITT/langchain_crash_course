{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c26845d",
   "metadata": {},
   "source": [
    "# Langchain crash course"
   ]
  },
  {
   "cell_type": "code",
   "id": "f31c4cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T14:49:15.880469Z",
     "start_time": "2025-06-19T14:49:15.876946Z"
    }
   },
   "source": [
    "# from secret_key import openapi_key\n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = openapi_key\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "9ed0dc6a",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa352d5f",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T14:50:34.742045Z",
     "start_time": "2025-06-19T14:50:14.662191Z"
    }
   },
   "source": [
    "from langchain.llms import OpenAI\n",
    "#from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "'''llm = ChatOpenAI(\n",
    "    model='deepseek-chat',\n",
    "    openai_api_key='',\n",
    "    openai_api_base='https://api.deepseek.com',\n",
    "    max_tokens=1024,\n",
    "    temperature=0.9\n",
    ")'''\n",
    "\n",
    "name = llm.invoke(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='That sounds like a fantastic idea! Here are some fancy and creative name ideas for your Indian restaurant:  \\n\\n### **Elegant & Sophisticated Names:**  \\n1. **Saffron & Silk** â€“ Evokes luxury and rich Indian flavors.  \\n2. **Royal Masala** â€“ A regal touch for authentic Indian cuisine.  \\n3. **The Spice Emporium** â€“ Perfect for a high-end Indian dining experience.  \\n4. **Mughalâ€™s Feast** â€“ Inspired by the grandeur of Mughlai cuisine.  \\n5. **Biryani & Beyond** â€“ Upscale yet inviting, focusing on Indiaâ€™s beloved dish.  \\n\\n### **Modern & Trendy Names:**  \\n6. **Curry & Karma** â€“ A playful yet stylish twist.  \\n7. **Tandoori Nights** â€“ Chic and exotic, great for a fine-dining vibe.  \\n8. **Namaste Gourmet** â€“ Blends tradition with a contemporary feel.  \\n9. **Spice Route** â€“ A nod to Indiaâ€™s historic spice trade.  \\n10. **The Turmeric Table** â€“ Fresh, vibrant, and upscale.  \\n\\n### **Luxury & Exotic Names:**  \\n11. **Maharajaâ€™s Kitchen** â€“ Fit for royalty.  \\n12. **Opal & Oudh** â€“ A fusion of Indian gemstones and rich heritage.  \\n13. **Saffron Sky** â€“ Dreamy and elegant.  \\n14. **The Golden Thali** â€“ A premium take on the classic Indian platter.  \\n15. **Kohinoor Dining** â€“ Named after the legendary diamond, symbolizing luxury.  \\n\\nWould you like a name that leans more toward **traditional, modern, or fusion**? Also, do you want it to reflect a specific regional cuisine (e.g., Punjabi, South Indian, Mughlai)? Let me know, and I can refine the suggestions further! ðŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 404, 'prompt_tokens': 21, 'total_tokens': 425, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 21}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0425fp8', 'id': 'af30ccb3-a5ab-4177-a042-dcb62fed110c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--9587c8f3-ef15-4c8e-a64f-fe902f58ed9a-0' usage_metadata={'input_tokens': 21, 'output_tokens': 404, 'total_tokens': 425, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "b56e8581",
   "metadata": {},
   "source": [
    "llm(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0782a2dd",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "id": "7a306b9d",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T14:28:27.160722Z",
     "start_time": "2025-06-19T14:28:27.103494Z"
    }
   },
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "p = prompt_template_name.format(cuisine=\"Italian\")\n",
    "print(p)\n",
    "\n",
    "# print(llm.predict(p))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Italian food. Suggest a fency name for this.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "af406b92",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba65c213",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T14:30:49.163132Z",
     "start_time": "2025-06-19T14:30:36.123735Z"
    }
   },
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "chain.run(\"Mexican\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are some fancy and creative name ideas for your Mexican restaurant:  \\n\\n1. **Sabor Elegante** â€“ \"Elegant Flavor\"  \\n2. **Mestizo** â€“ A nod to mixed heritage (Spanish & Indigenous)  \\n3. **La Esmeralda** â€“ \"The Emerald\" (suggests luxury)  \\n4. **Don Julioâ€™s Reserve** â€“ Upscale & personal  \\n5. **Azul Cantina** â€“ \"Blue Tavern\" (stylish & modern)  \\n6. **Fuego & Oro** â€“ \"Fire & Gold\" (bold & luxurious)  \\n7. **El Dorado Cocina** â€“ Mythical \"Golden\" cuisine  \\n8. **Hacienda Real** â€“ \"Royal Estate\"  \\n9. **Quetzal** â€“ Named after the sacred bird (symbol of elegance)  \\n10. **Tres Lunas** â€“ \"Three Moons\" (mystical & refined)  \\n\\nWould you like a name that leans more traditional, modern, or high-end luxury? Let me know, and I can refine the suggestions further!  \\n\\nÂ¡Buen provecho! ðŸŒ®ðŸ¹'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "e5ccee75",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T14:31:02.758054Z",
     "start_time": "2025-06-19T14:30:49.184315Z"
    }
   },
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, verbose=True)\n",
    "chain.run(\"Mexican\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mI want to open a restaurant for Mexican food. Suggest a fency name for this.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That sounds like a fantastic venture! Here are some fancy and stylish name ideas for your Mexican restaurant:  \\n\\n1. **Sabor Elegante** â€“ \"Elegant Flavor\"  \\n2. **Mestizo** â€“ A nod to mixed heritage, representing rich Mexican culture  \\n3. **Azul Cantina** â€“ \"Blue Tavern,\" evoking a chic, coastal vibe  \\n4. **La Esmeralda** â€“ \"The Emerald,\" symbolizing luxury and rarity  \\n5. **Don Julioâ€™s Reserve** â€“ A high-end twist on a classic name  \\n6. **Oro y Plata** â€“ \"Gold and Silver,\" representing refinement  \\n7. **Fuego & Flor** â€“ \"Fire & Flower,\" balancing spice and elegance  \\n8. **El Dorado Cocina** â€“ \"The Golden Kitchen,\" suggesting exclusivity  \\n9. **Hacienda Real** â€“ \"Royal Estate,\" evoking grandeur  \\n10. **Luna de Miel** â€“ \"Honeymoon,\" for a romantic and upscale feel  \\n\\nWould you like a name that leans more traditional, modern, or fusion-inspired? I can refine the suggestions based on your vision!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "21098937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T14:32:53.320241Z",
     "start_time": "2025-06-19T14:32:53.316179Z"
    }
   },
   "source": [
    "llm = OpenAI(temperature=0.6)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose=False prompt=PromptTemplate(input_variables=['restaurant_name'], input_types={}, partial_variables={}, template='Suggest some menu items for {restaurant_name}') llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000022980A0C4D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000022980B74BD0>, root_client=<openai.OpenAI object at 0x0000022980B5E490>, root_async_client=<openai.AsyncOpenAI object at 0x0000022980B5FED0>, temperature=0.6, model_kwargs={}, openai_api_key=SecretStr('')) output_parser=StrOutputParser() llm_kwargs={}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "87a98d9f",
   "metadata": {},
   "source": [
    "#### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9fd9a79",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-06-19T14:38:55.089499Z",
     "start_time": "2025-06-19T14:38:53.548927Z"
    }
   },
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "\n",
    "content = chain.run(\"Indian\")\n",
    "print(content)"
   ],
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLocalProtocolError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001B[0m, in \u001B[0;36mmap_httpcore_exceptions\u001B[1;34m()\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 101\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[1;32m--> 250\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool\u001B[38;5;241m.\u001B[39mhandle_request(req)\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[1;32m--> 256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    235\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[1;32m--> 236\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39mhandle_request(\n\u001B[0;32m    237\u001B[0m         pool_request\u001B[38;5;241m.\u001B[39mrequest\n\u001B[0;32m    238\u001B[0m     )\n\u001B[0;32m    239\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[0;32m    242\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    243\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[1;32m--> 103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection\u001B[38;5;241m.\u001B[39mhandle_request(request)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    135\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_closed()\n\u001B[1;32m--> 136\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:86\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msend_request_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs\n\u001B[0;32m     85\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m---> 86\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_request_headers(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msend_request_body\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs) \u001B[38;5;28;01mas\u001B[39;00m trace:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:144\u001B[0m, in \u001B[0;36mHTTP11Connection._send_request_headers\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    142\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m--> 144\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions({h11\u001B[38;5;241m.\u001B[39mLocalProtocolError: LocalProtocolError}):\n\u001B[0;32m    145\u001B[0m     event \u001B[38;5;241m=\u001B[39m h11\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[0;32m    146\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    147\u001B[0m         target\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39murl\u001B[38;5;241m.\u001B[39mtarget,\n\u001B[0;32m    148\u001B[0m         headers\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m    149\u001B[0m     )\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\contextlib.py:158\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[1;34m(self, typ, value, traceback)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgen\u001B[38;5;241m.\u001B[39mthrow(typ, value, traceback)\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001B[0m, in \u001B[0;36mmap_exceptions\u001B[1;34m(map)\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exc, from_exc):\n\u001B[1;32m---> 14\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m to_exc(exc) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mLocalProtocolError\u001B[0m: Illegal header value b'Bearer '",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mLocalProtocolError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:972\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[0;32m    971\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 972\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39msend(\n\u001B[0;32m    973\u001B[0m         request,\n\u001B[0;32m    974\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_stream_response_body(request\u001B[38;5;241m=\u001B[39mrequest),\n\u001B[0;32m    975\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    976\u001B[0m     )\n\u001B[0;32m    977\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:914\u001B[0m, in \u001B[0;36mClient.send\u001B[1;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[0;32m    912\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[1;32m--> 914\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_handling_auth(\n\u001B[0;32m    915\u001B[0m     request,\n\u001B[0;32m    916\u001B[0m     auth\u001B[38;5;241m=\u001B[39mauth,\n\u001B[0;32m    917\u001B[0m     follow_redirects\u001B[38;5;241m=\u001B[39mfollow_redirects,\n\u001B[0;32m    918\u001B[0m     history\u001B[38;5;241m=\u001B[39m[],\n\u001B[0;32m    919\u001B[0m )\n\u001B[0;32m    920\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:942\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[1;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[0;32m    941\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 942\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_handling_redirects(\n\u001B[0;32m    943\u001B[0m         request,\n\u001B[0;32m    944\u001B[0m         follow_redirects\u001B[38;5;241m=\u001B[39mfollow_redirects,\n\u001B[0;32m    945\u001B[0m         history\u001B[38;5;241m=\u001B[39mhistory,\n\u001B[0;32m    946\u001B[0m     )\n\u001B[0;32m    947\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:979\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[1;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[0;32m    977\u001B[0m     hook(request)\n\u001B[1;32m--> 979\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_single_request(request)\n\u001B[0;32m    980\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:1014\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m   1013\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[1;32m-> 1014\u001B[0m     response \u001B[38;5;241m=\u001B[39m transport\u001B[38;5;241m.\u001B[39mhandle_request(request)\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:249\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    237\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[0;32m    238\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    239\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    247\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[0;32m    248\u001B[0m )\n\u001B[1;32m--> 249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m    250\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool\u001B[38;5;241m.\u001B[39mhandle_request(req)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\contextlib.py:158\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[1;34m(self, typ, value, traceback)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgen\u001B[38;5;241m.\u001B[39mthrow(typ, value, traceback)\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001B[0m, in \u001B[0;36mmap_httpcore_exceptions\u001B[1;34m()\u001B[0m\n\u001B[0;32m    117\u001B[0m message \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(exc)\n\u001B[1;32m--> 118\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m mapped_exc(message) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[1;31mLocalProtocolError\u001B[0m: Illegal header value b'Bearer '",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mAPIConnectionError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchains\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SimpleSequentialChain\n\u001B[0;32m      2\u001B[0m chain \u001B[38;5;241m=\u001B[39m SimpleSequentialChain(chains \u001B[38;5;241m=\u001B[39m [name_chain, food_items_chain])\n\u001B[1;32m----> 4\u001B[0m content \u001B[38;5;241m=\u001B[39m chain\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndian\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(content)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    189\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    190\u001B[0m     emit_warning()\n\u001B[1;32m--> 191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:603\u001B[0m, in \u001B[0;36mChain.run\u001B[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[0;32m    601\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    602\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 603\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[0;32m    604\u001B[0m         _output_key\n\u001B[0;32m    605\u001B[0m     ]\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[0;32m    609\u001B[0m         _output_key\n\u001B[0;32m    610\u001B[0m     ]\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    189\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    190\u001B[0m     emit_warning()\n\u001B[1;32m--> 191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:386\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \n\u001B[0;32m    356\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    377\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    379\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[0;32m    381\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[0;32m    384\u001B[0m }\n\u001B[1;32m--> 386\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m    387\u001B[0m     inputs,\n\u001B[0;32m    388\u001B[0m     cast(RunnableConfig, {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m config\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m}),\n\u001B[0;32m    389\u001B[0m     return_only_outputs\u001B[38;5;241m=\u001B[39mreturn_only_outputs,\n\u001B[0;32m    390\u001B[0m     include_run_info\u001B[38;5;241m=\u001B[39minclude_run_info,\n\u001B[0;32m    391\u001B[0m )\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    166\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 167\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    168\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:157\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[0;32m    156\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 157\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    160\u001B[0m     )\n\u001B[0;32m    162\u001B[0m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    163\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[0;32m    164\u001B[0m     )\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\sequential.py:181\u001B[0m, in \u001B[0;36mSimpleSequentialChain._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m    179\u001B[0m color_mapping \u001B[38;5;241m=\u001B[39m get_color_mapping([\u001B[38;5;28mstr\u001B[39m(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchains))])\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, chain \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchains):\n\u001B[1;32m--> 181\u001B[0m     _input \u001B[38;5;241m=\u001B[39m chain\u001B[38;5;241m.\u001B[39mrun(\n\u001B[0;32m    182\u001B[0m         _input, callbacks\u001B[38;5;241m=\u001B[39m_run_manager\u001B[38;5;241m.\u001B[39mget_child(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    183\u001B[0m     )\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrip_outputs:\n\u001B[0;32m    185\u001B[0m         _input \u001B[38;5;241m=\u001B[39m _input\u001B[38;5;241m.\u001B[39mstrip()\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    189\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    190\u001B[0m     emit_warning()\n\u001B[1;32m--> 191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:603\u001B[0m, in \u001B[0;36mChain.run\u001B[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[0;32m    601\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    602\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 603\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[0;32m    604\u001B[0m         _output_key\n\u001B[0;32m    605\u001B[0m     ]\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[0;32m    609\u001B[0m         _output_key\n\u001B[0;32m    610\u001B[0m     ]\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    189\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    190\u001B[0m     emit_warning()\n\u001B[1;32m--> 191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:386\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \n\u001B[0;32m    356\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    377\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    379\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[0;32m    381\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[0;32m    384\u001B[0m }\n\u001B[1;32m--> 386\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m    387\u001B[0m     inputs,\n\u001B[0;32m    388\u001B[0m     cast(RunnableConfig, {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m config\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m}),\n\u001B[0;32m    389\u001B[0m     return_only_outputs\u001B[38;5;241m=\u001B[39mreturn_only_outputs,\n\u001B[0;32m    390\u001B[0m     include_run_info\u001B[38;5;241m=\u001B[39minclude_run_info,\n\u001B[0;32m    391\u001B[0m )\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    166\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 167\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    168\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:157\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[0;32m    156\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 157\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    160\u001B[0m     )\n\u001B[0;32m    162\u001B[0m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    163\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[0;32m    164\u001B[0m     )\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py:127\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    124\u001B[0m     inputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m    125\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    126\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m--> 127\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate([inputs], run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py:139\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[1;34m(self, input_list, run_manager)\u001B[0m\n\u001B[0;32m    137\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[1;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    140\u001B[0m         prompts,\n\u001B[0;32m    141\u001B[0m         stop,\n\u001B[0;32m    142\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    143\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs,\n\u001B[0;32m    144\u001B[0m     )\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[0;32m    147\u001B[0m         cast(\u001B[38;5;28mlist\u001B[39m, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[0;32m    148\u001B[0m     )\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    949\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    950\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    954\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    955\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    956\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 957\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    773\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[0;32m    774\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    775\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 776\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    777\u001B[0m                 m,\n\u001B[0;32m    778\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    779\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    780\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    781\u001B[0m             )\n\u001B[0;32m    782\u001B[0m         )\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1020\u001B[0m     result \u001B[38;5;241m=\u001B[39m generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1022\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m   1023\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m   1024\u001B[0m     )\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1026\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1071\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1069\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response\u001B[38;5;241m.\u001B[39mheaders)}\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1071\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpayload)\n\u001B[0;32m   1072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response, generation_info)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    285\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    882\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    883\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    884\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    922\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    923\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[0;32m    924\u001B[0m     validate_response_format(response_format)\n\u001B[1;32m--> 925\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[0;32m    926\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/chat/completions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    927\u001B[0m         body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[0;32m    928\u001B[0m             {\n\u001B[0;32m    929\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[0;32m    930\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[0;32m    931\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maudio\u001B[39m\u001B[38;5;124m\"\u001B[39m: audio,\n\u001B[0;32m    932\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrequency_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: frequency_penalty,\n\u001B[0;32m    933\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m: function_call,\n\u001B[0;32m    934\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctions\u001B[39m\u001B[38;5;124m\"\u001B[39m: functions,\n\u001B[0;32m    935\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogit_bias\u001B[39m\u001B[38;5;124m\"\u001B[39m: logit_bias,\n\u001B[0;32m    936\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: logprobs,\n\u001B[0;32m    937\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_completion_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_completion_tokens,\n\u001B[0;32m    938\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[0;32m    939\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[0;32m    940\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodalities\u001B[39m\u001B[38;5;124m\"\u001B[39m: modalities,\n\u001B[0;32m    941\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m: n,\n\u001B[0;32m    942\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparallel_tool_calls\u001B[39m\u001B[38;5;124m\"\u001B[39m: parallel_tool_calls,\n\u001B[0;32m    943\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m: prediction,\n\u001B[0;32m    944\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpresence_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: presence_penalty,\n\u001B[0;32m    945\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreasoning_effort\u001B[39m\u001B[38;5;124m\"\u001B[39m: reasoning_effort,\n\u001B[0;32m    946\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n\u001B[0;32m    947\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m: seed,\n\u001B[0;32m    948\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mservice_tier\u001B[39m\u001B[38;5;124m\"\u001B[39m: service_tier,\n\u001B[0;32m    949\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop,\n\u001B[0;32m    950\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstore\u001B[39m\u001B[38;5;124m\"\u001B[39m: store,\n\u001B[0;32m    951\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[0;32m    952\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream_options\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream_options,\n\u001B[0;32m    953\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[0;32m    954\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[0;32m    955\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[0;32m    956\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_logprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_logprobs,\n\u001B[0;32m    957\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[0;32m    958\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m: user,\n\u001B[0;32m    959\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweb_search_options\u001B[39m\u001B[38;5;124m\"\u001B[39m: web_search_options,\n\u001B[0;32m    960\u001B[0m             },\n\u001B[0;32m    961\u001B[0m             completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParamsStreaming\n\u001B[0;32m    962\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m stream\n\u001B[0;32m    963\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParamsNonStreaming,\n\u001B[0;32m    964\u001B[0m         ),\n\u001B[0;32m    965\u001B[0m         options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[0;32m    966\u001B[0m             extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[0;32m    967\u001B[0m         ),\n\u001B[0;32m    968\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mChatCompletion,\n\u001B[0;32m    969\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    970\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mStream[ChatCompletionChunk],\n\u001B[0;32m    971\u001B[0m     )\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1249\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1235\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1236\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1237\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1244\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1245\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1246\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1247\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1248\u001B[0m     )\n\u001B[1;32m-> 1249\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[1;32mD:\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1004\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1001\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1003\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRaising connection error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1004\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(request\u001B[38;5;241m=\u001B[39mrequest) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   1006\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[0;32m   1007\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHTTP Response: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1008\u001B[0m     request\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1012\u001B[0m     response\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m   1013\u001B[0m )\n\u001B[0;32m   1014\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest_id: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, response\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx-request-id\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[1;31mAPIConnectionError\u001B[0m: Connection error."
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "0386d05c",
   "metadata": {},
   "source": [
    "#### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "id": "49dc0fae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T14:33:24.013660Z",
     "start_time": "2025-06-19T14:33:24.010557Z"
    }
   },
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "9dea8402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T14:34:28.837728Z",
     "start_time": "2025-06-19T14:34:28.834172Z"
    }
   },
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "1ec1be10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T14:34:44.543029Z",
     "start_time": "2025-06-19T14:34:44.540726Z"
    }
   },
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "4653c540",
   "metadata": {},
   "source": [
    "chain({\"cuisine\": \"Indian\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4069a75e",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e6ee719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure yoy have installed this package: pip install google-search-results\n",
    "# from secret_key import serpapi_key\n",
    "# os.environ['SERPAPI_API_KEY'] = serpapi_key\n",
    "\n",
    "os.environ['SERPAPI_API_KEY'] = \"add your serpapi key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b2c6b",
   "metadata": {},
   "source": [
    "#### serpapi and llm-math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec4212d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new  chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m I need to find the GDP of US in 2022\n",
      "Action: Search\n",
      "Action Input: US GDP in 2022\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3m$25.46 trillion\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I need to add 5 to this number\n",
      "Action: Calculator\n",
      "Action Input: 25.46 + 5\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mAnswer: 30.46\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I now know the final answer\n",
      "Final Answer: The GDP of US in 2022 plus 5 is $30.46 trillion.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The GDP of US in 2022 plus 5 is $30.46 trillion.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the GDP of US in 2022 plus 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd3a12",
   "metadata": {},
   "source": [
    "#### Wikipedia and llm-math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d06ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new  chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m I need to find out when Elon Musk was born and then calculate his age.\n",
      "Action: Wikipedia\n",
      "Action Input: Elon Musk\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3mPage: Elon Musk\n",
      "Summary: Elon Reeve Musk ( EE-lon; born June 28, 1971) is a business magnate and investor. He is the founder, CEO, and chief engineer of SpaceX; angel investor, CEO, and product architect of Tesla, Inc.; owner, CTO, and chairman of Twitter; founder of the Boring Company and X Corp.; co-founder of Neuralink and OpenAI; and president of the philanthropic Musk Foundation. Musk is the wealthiest person in the world according to the Bloomberg Billionaires Index and Forbes's Real Time Billionaires list as of June 2023, primarily from his ownership stakes in Tesla and SpaceX, with an estimated net worth of around $225 billion according to Bloomberg and $230.2 billion according to Forbes.Musk was born in Pretoria, South Africa, and briefly attended the University of Pretoria before moving to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University and transferred to the University of Pennsylvania, where he received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University. After two days, he dropped out and, with his brother Kimbal, co-founded the online city guide software company Zip2. In 1999, Zip2 was acquired by Compaq for $307 million and Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal, which eBay acquired for $1.5 billion in 2002.\n",
      "With $175.8 million, Musk founded SpaceX in 2002, a spaceflight services company. In 2004, he was an early investor in the electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. In 2006, he helped create SolarCity, a solar energy company that was later acquired by Tesla and became Tesla Energy. In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. The following year, he co-founded Neuralinkâ€”a neurotechnology company developing brainâ€“computer interfacesâ€”and the Boring Company, a tunnel construction company. Musk has also proposed a hyperloop high-speed vactrain transportation system. In 2022, his acquisition of Twitter for $44 billion was completed. \n",
      "Musk has expressed views that have made him a polarizing figure. He has been criticized for making unscientific and misleading statements, including that of spreading COVID-19 misinformation. In 2018, the U.S. Securities and Exchange Commission (SEC) sued Musk for falsely tweeting that he had secured funding for a private takeover of Tesla. Musk stepped down as chairman of Tesla and paid a $20 million fine as part of a settlement agreement with the SEC.\n",
      "\n",
      "\n",
      "\n",
      "Page: Acquisition of Twitter by Elon Musk\n",
      "Summary: Business magnate Elon Musk initiated an acquisition of American social media company Twitter, Inc. on April 14, 2022, and concluded it on October 27, 2022. Musk had begun buying shares of the company in January 2022, becoming its largest shareholder by April with a 9.1 percent ownership stake. Twitter invited Musk to join its board of directors, an offer he initially accepted before declining. On April 14, Musk made an unsolicited offer to purchase the company, to which Twitter's board initially responded with a \"poison pill\" strategy to resist a hostile takeover, before unanimously accepting Musk's buyout offer of $44 billion on April 25. Musk stated that he planned to introduce new features to the platform, make its algorithms open-source, combat spambot accounts, and promote free speech.\n",
      "In July, Musk announced his intention to terminate the agreement, asserting that Twitter had breached their agreement by refusing to crack down on spambot accounts. The company filed a lawsuit against Musk in the Delaware Court of Chancery shortly thereafter, with a trial scheduled for the week of October 17. Weeks before the trial was set to begin, Musk reversed course, announcing that he would move forward with the acquisition. The deal was closed on October 27, with\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I now know the final answer\n",
      "Final Answer: Elon Musk was born on June 28, 1971 and is currently 52 years old in 2023.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Elon Musk was born on June 28, 1971 and is currently 52 years old in 2023.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install this package: pip install wikipedia\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"When was Elon musk born? What is his age right now in 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be7ee7",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2acab5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Taco Fiesta\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bc200f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Maharaja's Palace Cuisine\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "229a6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f492fb5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871492be",
   "metadata": {},
   "source": [
    "#### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53eea298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Taco Fiesta\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0de5d50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Al-Fez Restaurant\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Arabic\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cc88888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "Taco Fiesta\n",
      "Human: Arabic\n",
      "AI: \n",
      "\n",
      "Al-Fez Restaurant\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a88b5b",
   "metadata": {},
   "source": [
    "#### ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "687ddd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47ad5062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first Cricket World Cup was won by the West Indies in 1975.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03c80b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 10.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07342f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the West Indies team that won the first Cricket World Cup in 1975 was Clive Lloyd.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain ofthe winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e459d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who won the first cricket world cup?\n",
      "AI:  The first Cricket World Cup was won by the West Indies in 1975.\n",
      "Human: How much is 5+5?\n",
      "AI:  10.\n",
      "Human: Who was the captain ofthe winning team?\n",
      "AI:  The captain of the West Indies team that won the first Cricket World Cup in 1975 was Clive Lloyd.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa3abd",
   "metadata": {},
   "source": [
    "#### ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "460eb33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first Cricket World Cup was held in 1975 and the winner was the West Indies.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=memory\n",
    ")\n",
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d395beaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5+5 is 10.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93b24745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I don't know.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
